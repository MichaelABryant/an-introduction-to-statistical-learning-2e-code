abline(lm12.fit, col=2)
summary(lm13.fit)
plot(medv, crim)
abline(lm13.fit, col=2)
# all predictors
lmfull.fit <- lm(crim ~ .)
# all predictors
lmfull.fit <- lm(crim ~ ., data=Boston)
summary(lmfull.fit)
lmfull.fit$coefficients
lmfull.fit$coefficients[0]
lmfull.fit$coefficients[1]
lmfull.fit$coefficients[[1]]
for(i in 1:14){
out[[i]] <- lmfull.fit$coefficients[[i]]
}
out <- []
out <- 0
for(i in 1:14){
out[[i]] <- lmfull.fit$coefficients[[i]]
}
lmfull.fit$coefficients[[1]]
lmfull.fit$coefficients[1]
for(i in 2:14){
out[[i]] <- lmfull.fit$coefficients[[i]]
}
out <- 0
for(i in 2:14){
out[[i]] <- lmfull.fit$coefficients[[i]]
}
lmfull.fit$coefficients[[2]]
out <- 0
for(i in 2:14){
out[[i-1]] <- lmfull.fit$coefficients[[i]]
}
out <- 0
for(i in 2:14){
out[[i-1]] <- lmfull.fit$coefficients[[i]]
}
lm.fit$coefficients
lm.fit$coefficients[[1]]
lm.fit$coefficients[[2]]
uni <- lm.fit$coefficients[[2]]
uni <- lm1.fit$coefficients[[2]] + lm2.fit$coefficients[[2]]
uni <- array(lm1.fit$coefficients[[2]],
lm2.fit$coefficients[[2]],
lm3.fit$coefficients[[2]],
lm4.fit$coefficients[[2]],
lm5.fit$coefficients[[2]],
lm6.fit$coefficients[[2]],
lm7.fit$coefficients[[2]],
lm8.fit$coefficients[[2]],
lm9.fit$coefficients[[2]],
lm10.fit$coefficients[[2]],
lm11.fit$coefficients[[2]],
lm12.fit$coefficients[[2]],
lm13.fit$coefficients[[2]])
lm4.fit$coefficients[[2]]
uni <- array(c(lm1.fit$coefficients[[2]],
lm2.fit$coefficients[[2]],
lm3.fit$coefficients[[2]],
lm4.fit$coefficients[[2]],
lm5.fit$coefficients[[2]],
lm6.fit$coefficients[[2]],
lm7.fit$coefficients[[2]],
lm8.fit$coefficients[[2]],
lm9.fit$coefficients[[2]],
lm10.fit$coefficients[[2]],
lm11.fit$coefficients[[2]],
lm12.fit$coefficients[[2]],
lm13.fit$coefficients[[2]]))
plot(uni, out)
lmfull.fit$coefficients[[2]]
lmfull.fit$coefficients[2]
lm1.fit$coefficients[[2]]
lm1.fit$coefficients[2]
identify(uni, out, name)
plot(uni, out)
identify(uni, out, name)
identify(uni, out)
lm1.fit <- lm(crim ~ poly(zn,3))
lm2.fit <- lm(crim ~ poly(indus,3))
lm3.fit <- lm(crim ~ poly(chas,3))
lm4.fit <- lm(crim ~ poly(nox,3))
lm5.fit <- lm(crim ~ poly(rm,3))
lm6.fit <- lm(crim ~ poly(age,3))
lm7.fit <- lm(crim ~ poly(dis,3))
lm8.fit <- lm(crim ~ poly(rad,3))
lm9.fit <- lm(crim ~ poly(tax,3))
lm10.fit <- lm(crim ~ poly(ptratio,3))
lm11.fit <- lm(crim ~ poly(black,3))
lm12.fit <- lm(crim ~ poly(lstat,3))
lm13.fit <- lm(crim ~ poly(medv,3))
plot(crim, chas)
lm1.fit <- lm(crim ~ poly(zn,3))
lm2.fit <- lm(crim ~ poly(indus,3))
lm4.fit <- lm(crim ~ poly(nox,3))
lm5.fit <- lm(crim ~ poly(rm,3))
lm6.fit <- lm(crim ~ poly(age,3))
lm7.fit <- lm(crim ~ poly(dis,3))
lm8.fit <- lm(crim ~ poly(rad,3))
lm9.fit <- lm(crim ~ poly(tax,3))
lm10.fit <- lm(crim ~ poly(ptratio,3))
lm11.fit <- lm(crim ~ poly(black,3))
lm12.fit <- lm(crim ~ poly(lstat,3))
lm13.fit <- lm(crim ~ poly(medv,3))
summary(lm1.fit)
plot(zn, crim)
abline(lm1.fit, col=2)
summary(lm1.fit)
summary(lm2.fit)
summary(lm4.fit)
summary(lm5.fit)
summary(lm6.fit)
summary(lm7.fit)
summary(lm8.fit)
summary(lm9.fit)
summary(lm11.fit)
summary(lm12.fit)
summary(lm13.fit)
lm1.fit <- lm(crim ~ poly(zn,2))
#all but chas have est. coef. which are statistically significant
summary(lm1.fit)
summary(lm2.fit)
lm2.fit <- lm(crim ~ poly(indus,4))
summary(lm2.fit)
lm2.fit <- lm(crim ~ poly(indus,3))
summary(lm2.fit)
summary(lm1.fit)
summary(lm2.fit)
summary(lm4.fit)
summary(lm5.fit)
summary(lm6.fit)
summary(lm7.fit)
summary(lm8.fit)
summary(lm9.fit)
summary(lm11.fit)
summary(lm12.fit)
summary(lm13.fit)
lm1.fit <- lm(crim ~ poly(zn,3))
lm2.fit <- lm(crim ~ poly(indus,3))
lm4.fit <- lm(crim ~ poly(nox,3))
lm5.fit <- lm(crim ~ poly(rm,3))
lm6.fit <- lm(crim ~ poly(age,3))
lm7.fit <- lm(crim ~ poly(dis,3))
lm8.fit <- lm(crim ~ poly(rad,3))
lm9.fit <- lm(crim ~ poly(tax,3))
lm10.fit <- lm(crim ~ poly(ptratio,3))
lm11.fit <- lm(crim ~ poly(black,3))
lm12.fit <- lm(crim ~ poly(lstat,3))
lm13.fit <- lm(crim ~ poly(medv,3))
#all but chas have est. coef. which are statistically significant
summary(lm1.fit)
summary(lm2.fit)
summary(lm4.fit)
summary(lm5.fit)
summary(lm6.fit)
summary(lm7.fit)
summary(lm8.fit)
summary(lm9.fit)
summary(lm11.fit)
summary(lm12.fit)
summary(lm13.fit)
rm(list=ls())
s_market <- read.csv("Smarket.csv")
getwd()
setwd("C:/Users/malex/Documents/Introduction to Statistical Learning Labs/Chapter 4/Lab")
s_market <- read.csv("Smarket.csv")
view(s_market)
View(s_market)
s_market <- read.csv("Smarket.csv", stringsAsFactors = TRUE)
View(s_market)
which(is.na(s_market))
names(Smarket)
names(S_market)
names(s_market)
dim(s_market)
summary(s_market)
#scatter matrix
pairs(s_market)
#scatter matrix
pairs(s_market, col=2)
cor(s_market[ ,-9])
attach(s_market)
plot(Volume)
plot(Year, Volume)
plot(Volume)
#logistic regression fit (glm is generalized linear model)
glm.fits <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = s_market,
family = binomial)
summary(glm.fits)
coef(glm.fits)
summary(glm.fits)$coef
summary(glm.fits$coef[,4])
summary(glm.fits)$coef[,4]
#make predictions of the response (type can be set to indicate logit, etc.)
glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:10]
contrasts(Direction)
glm.pred <- rep("Down", 1250)
glm.pred[glm.probs > .5] = "Up"
table(glm.pred, Direction)
(507+145)/1250
mean(glm.pred == Direction)
mean(Direction)
summary(Direction)
648/(648+602)
#however, training set accuracy may overfit, so a hold out set is necessary
#to estimate real world performance
train <- (Year < 2005)
s_market.2005 <- s_market[!train,]
dim(smarket.2005)
dim(s_market.2005)
Direction.2005 <- Direction[!train]
dim(Direction.2005)
Direction.2005 <- Direction[!train]
dim(Direction.2005)
dim(s_market.2005)
dim(train)
#save 2005 as a holdout set
train <- (Year < 2005)
dim(train)
#fit logistic regression model to training data
glm.fits <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = s_market,
family = binomial,
subset = train)
glm.probs <- predict(glm.fits,
Smarket.2005,
type = "response")
glm.probs <- predict(glm.fits,
s_market.2005,
type = "response")
glm.pred <- rep("Down", 252)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
mean(glm.pred != Direction.2005)
#predict for specific values of Lag1 and Lag2
predict(glm.fits,
newdata = data.frame(Lag1 = c(1.2,1.5), Lag2 = c(1.1, -0.8)),
type = "response")
#predict for specific values of Lag1 and Lag2
predict(glm.fits,
data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),
type = "response")
new <- data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8))
#predict for specific values of Lag1 and Lag2
predict(glm.fits,
newdata = new,
type = "response")
#fit logistic regression model to training data
glm.fits <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
data = s_market,
family = binomial,
subset = train)
#predict for specific values of Lag1 and Lag2
predict(glm.fits,
newdata = new,
type = "response")
#fit logistic regression model to training data
glm.fits <- glm(Direction ~ Lag1 + Lag2,
data = s_market,
family = binomial,
subset = train)
#predict on hold out set
glm.probs <- predict(glm.fits,
s_market.2005,
type = "response")
#create a vector the size of the hold out set with "Down" elements
glm.pred <- rep("Down", 252)
#replace "Down" with "Up" where probabilities are greater than .5
glm.pred[glm.probs > .5] <- "Up"
#confusion matrix
table(glm.pred, Direction.2005)
#accuracy
mean(glm.pred == Direction.2005)
#error rate
mean(glm.pred != Direction.2005)
106/(76+106)
#predict for specific values of Lag1 and Lag2
predict(glm.fits,
newdata = data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),
type = "response")
lda.fit <- lda(Direction ~ Lag1 + Lag2,
data = s_market,
subset = train)
library(MASS)
lda.fit <- lda(Direction ~ Lag1 + Lag2,
data = s_market,
subset = train)
lda.fit
plot(lda.fit)
lda.pred <- predict(lda.fit, s_market.2005)
names(lda.pred)
lda.class <- lda.pred$class
table(lda.class, Direction.2005)
mean(lda.class = Direction.2005)
mean(lda.class == Direction.2005)
sum(lda.pred$posterior[,1] >= .5)
sum(lda.pred$posterior[,1] < .5)
lda.pred$posterior[1:20,1]
lda.class[1:20]
sum(lda.pred$posterior[,1] > .9)
max(lda.pred$posterior[,1])
#qda is a part of the MASS library
qda.fit <- qda(Direction ~ Lag1 + Lag2,
data = s_market,
subset = train)
qda.fit
qda.class <- predict(qda.fit, s_market.2005)
table(qda.class, Direction.2005)
qda.class <- predict(qda.fit, s_market.2005)
table(qda.class, Direction.2005)
qda.class <- predict(qda.fit, s_market.2005)$class
table(qda.class, Direction.2005)
mean(qda.class == Direction.2005)
#import library with naive Bayes
library(e1071)
#models each quantitative feature using Gaussian distribution, by default
nb.fit <- naiveBayes(Direction ~ Lag1 + Lag2,
data = s_market,
subset = train)
nb.fit
mean(Lag1[train][Direction[train] == "Down"])
sd(Lag1[train][Direction[train] == "Down"])
nb.class <- prediction(nb.fit, s_market.2005)
table(nb.class, Direction.2005)
nb.class <- prediction(nb.fit, s_market.2005)$class
nb.class <- prediction(nb.fit, s_market.2005)
nb.class <- predict(nb.fit, s_market.2005)
table(nb.class, Direction.2005)
mean(nb.class == Direction.2005)
#estimates of probabilities
nb.preds <- predict(nb.fit, s_market.2005, type = 'raw')
nb.preds[1:5,]
#import library with knn
library(class)
#create training and test matricies and train-target array
#cbind = column bind
train.X <- cbind(Lag1,Lag2)[train,]
test.X <- cbind(Lag1,Lag2)[!train,]
train.Direction <- Direction[train]
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Direction, k=1)
table(knn.pred, Direction.2005)
83/(68+83)
(83+43)/252
#use k = 3
knn.pred <- knn(train.X, test.X, train.Direction, k=3)
table(knn.pred, Direction.2005)
(48+87)/252
#moving onto Caravan data set
caravan <- read.csv("Caravan.csv", stringsAsFactors = TRUE)
attach(caravan)
summary(Purchase)
#knn uses distance for predictions, therefore need to standardize
standardize.X <- scale(Caravan[,-86])
#knn uses distance for predictions, therefore need to standardize (excluding Purchase)
standardize.X <- scale(caravan[,-86])
var(caravan[,1])
var(caravan[,2])
var(standardized.X[,1])
#knn uses distance for predictions, therefore need to standardize (excluding Purchase)
standardized.X <- scale(caravan[,-86])
View(standardize.X)
var(standardized.X[,1])
var(standardized.X[,2])
#train-test split
#first 1000 test, rest train
test <- 1:1000
train.X <- standardized.X[-test,]
test.X <-standardized.X[test,]
train.Y <- Purchase[-test]
test.Y <- Purchase[test]
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Y, k = 1)
mean(test.Y != knn.pred)
mean(test.Y != "No")
count.fields(test.Y != "No") #5.9% of test is
summary(test.Y)
59/(941+59)
#if the company is only interested in targeting individuals who are likely to buy,
#then error rate/accuracy does not matter, instead what matters is sensitivity (TP rate)
table(knn.pred, test.Y)
9/(9+68)
knn.pred <- knn(train.X, test.X, train.Y, k = 3)
table(knn.pred, test.Y)
5/(21+5)
knn.pred <- knn(train.X, test.X, train.Y, k = 5)
table(knn.pred, test.Y)
4/(11+4)
59/(941+59)
26.7/5.9
#comparison to logistic regression
glm.fits <- glm(Purchase ~., data = caravan, family = binomial, subset = -test)
glm.probs <- predict(glm.fits, Caravan[test,])
glm.probs <- predict(glm.fits, Caravan[test,], type = "response")
glm.probs <- predict(glm.fits, caravan[test,], type = "response")
glm.pred <- rep("No", 1000)
glm.pred[glm.probs >.5] <- "Yes"
table(glm.pred, test.Y)
0/(0+7)
#changing the threshold to 0.25
glm.pred <- rep("No", 1000)
glm.pred[glm.probs >.25] <- "Yes"
table(glm.pred, test.Y)
11/(22+11)
bikeshare <- read.csv("Bikeshare.csv")
View(bikeshare)
View(bikeshare)
bikeshare <- read.csv("Bikeshare.csv", stringsAsFactors = TRUE)
dim(bikeshare)
names(bikeshare)
View(bikeshare)
View(bikeshare)
bikeshare <- read.csv("Bikeshare.csv", row.names = "X",stringsAsFactors = TRUE)
dim(bikeshare)
names(bikeshare)
#least squares
mod.lm <- lm(bikers ~ mnth + hr + workingday + temp + weathersit,
data = bikeshare)
summary(mod.lm)
bikeshare <- read.csv("Bikeshare.csv", row.names = "X")
#least squares
mod.lm <- lm(bikers ~ mnth + hr + workingday + temp + weathersit,
data = bikeshare)
summary(mod.lm)
bikeshare <- read.csv("Bikeshare.csv", row.names = "X", ,stringsAsFactors = TRUE)
#least squares
mod.lm <- lm(bikers ~ mnth + hr + workingday + temp + weathersit,
data = bikeshare)
summary(mod.lm)
bikeshare <- read.csv("Bikeshare.csv", row.names = "X",stringsAsFactors = TRUE)
#least squares
mod.lm <- lm(bikers ~ mnth + hr + workingday + temp + weathersit,
data = bikeshare)
summary(mod.lm)
bikeshare$hr <- as.factor(bikeshare$hr)
#least squares
mod.lm <- lm(bikers ~ mnth + hr + workingday + temp + weathersit,
data = bikeshare)
summary(mod.lm)
bikeshare$workingday <- as.factor(bikeshare$workingday)
#least squares
mod.lm <- lm(bikers ~ mnth + hr + workingday + temp + weathersit,
data = bikeshare)
summary(mod.lm)
library(ISLR2)
dim(Bikeshare)
names(Bikeshare)
#least squares
mod.lm <- lm(bikers ~ mnth + hr + workingday + temp + weathersit,
data = bikeshare)
summary(mod.lm)
#least squares
mod.lm <- lm(bikers ~ mnth + hr + workingday + temp + weathersit,
data = Bikeshare)
summary(mod.lm)
contrasts(Bikeshare$hr) = contr.sum(24)
contrasts(Bikeshare$mnth) = contr.sum(12)
mod.lm2 <- lm(bikers ~ mnth + hr + workingday + temp + weathersit,
data = Bikeshare)
summary(mod.lm2)
sum((predict(mod.lm) - predict(mode.lm2))^2)
sum((predict(mod.lm) - predict(mod.lm2))^2)
#alternative to above
all.equal(predict(mod.lm), predict(mod.lm2))
coef.months <- c(coef(mod.lm2)[2:12], -sum(coef(mod.lm2)[2:12])
coef.months <- c(coef(mod.lm2)[2:12], -sum(coef(mod.lm2)[2:12]))
coef.months <- c(coef(mod.lm2)[2:12], -sum(coef(mod.lm2)[2:12]))
plot(coef.months, xlab="Month", ylab="Coefficient", xaxt = "n", col="blue",
pch = 19, type = "o")
axis(side = 1, at 1:12, labels = c("J", "F", "M", "A", "M", "J", "J", "A",
"S", "O", "N", "D"))
axis(side = 1, at = 1:12, labels = c("J", "F", "M", "A", "M", "J", "J", "A",
"S", "O", "N", "D"))
#coef of hr
coef.hours <- c(coef(mod.lm2)[13:35], -sum(coef(mod.lm2)[13:35]))
#plot coefficients vs hours
plot(coef.hours, xlab = "Hour", ylab = "Coefficient", col = "blue", pch = 19,
type = "o")
#poisson regression
mod.pois <- glm(bikers ~ mnth + hr + workingday + temp + weathersit,
data = Bikeshare,
famil = poisson)
summary(mod.pois)
#coef of first 11 months and using neg sum to compute last month
coef.months <- c(coef(mod.pois)[2:12], -sum(coef(mod.lm2)[2:12]))
#plot coefficients vs months
plot(coef.months, xlab="Month", ylab="Coefficient", xaxt = "n", col="blue",
pch = 19, type = "o")
axis(side = 1, at = 1:12, labels = c("J", "F", "M", "A", "M", "J", "J", "A",
"S", "O", "N", "D"))
#coef of first 11 months and using neg sum to compute last month
coef.months <- c(coef(mod.pois)[2:12], -sum(coef(mod.pois)[2:12]))
#plot coefficients vs months
plot(coef.months, xlab="Month", ylab="Coefficient", xaxt = "n", col="blue",
pch = 19, type = "o")
axis(side = 1, at = 1:12, labels = c("J", "F", "M", "A", "M", "J", "J", "A",
"S", "O", "N", "D"))
#coef of hr
coef.hours <- c(coef(mod.pois)[13:35], -sum(coef(mod.pois)[13:35]))
#plot coefficients vs hours
plot(coef.hours, xlab = "Hour", ylab = "Coefficient", col = "blue", pch = 19,
type = "o")
#plot comparison of predictions between lm2 and pois
#must use "response" to get pred in exp(b0 + b1*X1 + ...) instead of b0 + b1*X1+...
#which is default
plot(predict(mod.lm2), predict(mod.pois, type="response"))
abline(0,1,col=2,lwd=3)
